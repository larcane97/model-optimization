input_channel: 3

depth_multiple: 1.0
width_multiple: 1.0

backbone:
    # [repeat, module, args]
    [
        # Conv argument: [out_channel, kernel_size, stride, padding_size]
        # if padding_size is not given or null, the padding_size will be auto adjusted as padding='SAME' in TensorFlow
        [1, Conv, [64, 3, 1, 2, 1, "GeLU"]],
        ## MDConv
          # out_planes,
          # expand_ratio,
          # kernel_size,
          # stride,
          # reduction_ratio=4,
          # drop_connect_rate=0.0,
        [1, MBConv, [96, 4, 3, 2]],
        [1, MBConv, [192, 4, 3, 2]],
        [1, MBConv, [384, 4, 3, 2]],
        ## FFN
        ### d_k, d_v, h,dropout=.1,isTransfer=False):
        # (out_channel: int, activation: Union[str, None])
        [1,ScaledDotProductAttention,[64,64,8,0.1,True,False]],
        [1,Linear,[384,"GeLU",True]],
        [1,MaxPool1d,[2,2]],
        [1,ScaledDotProductAttention,[64,64,8,0.1,False,False]],
        [1,Linear,[768,"GeLU",True]],
        [1,MaxPool1d,[2,2]],
        [1,ScaledDotProductAttention,[64,64,8,0.1,False,True]],      
        # [1,ScaledDotProductAttention,[64,64,8,0.1,False,False]],
        # [1,Linear,[384,"ReLU",True]],
        # [1,ScaledDotProductAttention,[64,64,8,0.1,False,False]],
        # [1,Linear,[384,"ReLU",True]],

        # [1,ScaledDotProductAttention,[64,64,8,0.1,False,False]],
        # [1,Linear,[768,"GeLU",True]],
        # [1,MaxPool1d,[384,2,2]]
        # [1,ScaledDotProductAttention,[64,64,8,0.1,False,False]],
        # [1,Linear,[768,"ReLU",True]],
  
        # [1,Linear,[768,"ReLU",True]],
        
        [1, Conv, [768, 1, 1]],
        [1, GlobalAvgPool, []],
        [1, Conv, [1280, 1, 1]],
        [1, Flatten, []],
        [1, Linear, [6]]
    ]
